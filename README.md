# BERT ç§»åŠ¨ç«¯æ–‡æœ¬è¯†åˆ«ç³»ç»Ÿ (BERT Text Classification & NER for Mobile)

æœ¬é¡¹ç›®æä¾›äº†ä¸€å¥—å®Œæ•´çš„è§£å†³æ–¹æ¡ˆï¼Œç”¨äºè®­ç»ƒ BERT æ¨¡å‹è¯†åˆ«æ–‡æœ¬ä¸­çš„â€œæ—¥ç¨‹/é—¹é’Ÿâ€æ„å›¾ï¼Œå¹¶æå–â€œæ—¶é—´ã€åœ°ç‚¹ã€å†…å®¹â€ç­‰å®ä½“ä¿¡æ¯ã€‚æœ€ç»ˆæ¨¡å‹é€šè¿‡ ONNX å¯¼å‡ºï¼Œå¯ç›´æ¥éƒ¨ç½²äº Android/iOS ç§»åŠ¨ç«¯å®ç°ç¦»çº¿æ¨ç†ã€‚

## ğŸ“‚ é¡¹ç›®ç»“æ„

```text
BERT_Project/
â”œâ”€â”€ data/                       # è®­ç»ƒæ•°æ®å­˜æ”¾ç›®å½•
â”‚   â”œâ”€â”€ classifier_train.jsonl  # æ„å›¾åˆ†ç±»è®­ç»ƒæ•°æ®
â”‚   â””â”€â”€ ner_train.jsonl         # å®ä½“æŠ½å–è®­ç»ƒæ•°æ® (BIO/JSONæ ¼å¼)
â”œâ”€â”€ models/                     # æ¨¡å‹è¾“å‡ºç›®å½• (è®­ç»ƒåçš„æ¨¡å‹å’Œå¯¼å‡ºæ–‡ä»¶)
â”‚   â”œâ”€â”€ classifier/             # è®­ç»ƒå¥½çš„åˆ†ç±»å™¨ PyTorch æ¨¡å‹
â”‚   â”œâ”€â”€ ner/                    # è®­ç»ƒå¥½çš„ NER PyTorch æ¨¡å‹
â”‚   â”œâ”€â”€ classifier.onnx         # [äº§ç‰©] ç§»åŠ¨ç«¯å¯ç”¨åˆ†ç±»æ¨¡å‹ (FP32, ~390MB)
â”‚   â”œâ”€â”€ ner.onnx                # [äº§ç‰©] ç§»åŠ¨ç«¯å¯ç”¨ NER æ¨¡å‹ (FP32, ~390MB)
â”‚   â”œâ”€â”€ classifier.quant.onnx   # [æ¨è] é‡åŒ–åçš„åˆ†ç±»æ¨¡å‹ (Int8, ~100MB)
â”‚   â””â”€â”€ ner.quant.onnx          # [æ¨è] é‡åŒ–åçš„ NER æ¨¡å‹ (Int8, ~100MB)
â”œâ”€â”€ train_classifier.py         # åˆ†ç±»æ¨¡å‹è®­ç»ƒè„šæœ¬
â”œâ”€â”€ train_ner.py                # NER æ¨¡å‹è®­ç»ƒè„šæœ¬
â”œâ”€â”€ export_onnx.py              # åˆ†ç±»æ¨¡å‹ ONNX å¯¼å‡ºè„šæœ¬
â”œâ”€â”€ export_ner_onnx.py          # NER æ¨¡å‹ ONNX å¯¼å‡ºè„šæœ¬
â”œâ”€â”€ clean_and_quantize.py       # [æ–°å¢] æ¨¡å‹é‡åŒ–è„šæœ¬ (å‹ç¼©æ¨¡å‹ä½“ç§¯)
â”œâ”€â”€ infer_pipeline.py           # å®Œæ•´é“¾è·¯æµ‹è¯•è„šæœ¬ (ä¸²è”ä¸¤çº§æ¨¡å‹)
â”œâ”€â”€ verify_onnx.py              # å•æ¨¡å‹éªŒè¯è„šæœ¬
â”œâ”€â”€ Dockerfile                  # å¼€å‘ç¯å¢ƒé•œåƒå®šä¹‰
â”œâ”€â”€ docker-compose.yml          # å®¹å™¨ç¼–æ’é…ç½®
â””â”€â”€ requirements.txt            # Python ä¾èµ–
```

## ğŸš€ å¿«é€Ÿå¼€å§‹ (æ¨èä½¿ç”¨ Docker)

æœ¬é¡¹ç›®å·²å°è£… Docker ç¯å¢ƒï¼Œæ— éœ€åœ¨æœ¬åœ°å®‰è£…å¤æ‚çš„ PyTorch/CUDA ç¯å¢ƒã€‚

### 1. å¯åŠ¨å¼€å‘ç¯å¢ƒ
åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹è¿è¡Œï¼š

```bash
docker compose up -d --build
```

### 2. è¿›å…¥å¼€å‘å®¹å™¨
```bash
docker compose exec bert-trainer bash
```

> **æ³¨æ„**ï¼šåç»­æ‰€æœ‰å‘½ä»¤å‡é»˜è®¤åœ¨å®¹å™¨å†…çš„ `/app` ç›®å½•ä¸‹æ‰§è¡Œã€‚

---

## ğŸ› ï¸ è¯¦ç»†è®­ç»ƒä¸éƒ¨ç½²æµç¨‹

### ç¬¬ä¸€æ­¥ï¼šå‡†å¤‡æ•°æ® (æ•°æ®å¢å¼º)

ä¸ºæå‡æ¨¡å‹ç²¾åº¦ï¼Œæˆ‘ä»¬æä¾›äº†æ•°æ®ç”Ÿæˆè„šæœ¬ï¼Œå¯ä»¥è‡ªåŠ¨ç”Ÿæˆå¤§é‡å¸¦æ ‡æ³¨çš„è®­ç»ƒæ•°æ®ã€‚

1. **ç”Ÿæˆè®­ç»ƒæ•°æ® (10,000æ¡)**ï¼š
   ```bash
   python generate_data.py
   ```
   *è¯¥è„šæœ¬ä¼šåœ¨ `data/` ç›®å½•ä¸‹ç”Ÿæˆ `classifier_train_large.jsonl` å’Œ `ner_train_large.jsonl`ã€‚*

2. **æ•°æ®æ ¼å¼è¯´æ˜**ï¼š
   - **åˆ†ç±»æ•°æ®** (`data/classifier_train_large.jsonl`):
     ```json
     {"text": "æ˜å¤©ä¸‹åˆ3ç‚¹åœ¨ä¼šè®®å®¤å¼€ä¼š", "label": 0}
     ```
     *(Labelå®šä¹‰: 0=æ—¥ç¨‹, 1=é—¹é’Ÿ, 2=å…¶ä»–)*

   - **NERæ•°æ®** (`data/ner_train_large.jsonl`):
     ```json
     {"tokens": ["æ˜", "å¤©", "å¼€", "ä¼š"], "ner_tags": [1, 2, 0, 0]}
     ```
     *(Tags: O=0, B-TIME=1, I-TIME=2, B-LOC=3, I-LOC=4, B-CONTENT=5, I-CONTENT=6)*

### ç¬¬äºŒæ­¥ï¼šè®­ç»ƒæ„å›¾åˆ†ç±»æ¨¡å‹ (Classifier)
è¯¥æ¨¡å‹ç”¨äºåˆ¤æ–­ç”¨æˆ·è¾“å…¥çš„å¥å­æ˜¯â€œæ—¥ç¨‹â€ã€â€œé—¹é’Ÿâ€è¿˜æ˜¯â€œå…¶ä»–â€ã€‚

```bash
python train_classifier.py \
  --train_file data/classifier_train_large.jsonl \
  --output_dir models/classifier \
  --epochs 3
```
*è¾“å‡ºï¼šæ¨¡å‹æ–‡ä»¶ä¿å­˜åœ¨ `models/classifier/`*

### ç¬¬ä¸‰æ­¥ï¼šè®­ç»ƒå®ä½“æŠ½å–æ¨¡å‹ (NER)
è¯¥æ¨¡å‹ç”¨äºä»å¥å­ä¸­æå–å…·ä½“çš„æ—¶é—´ã€åœ°ç‚¹å’Œäº‹ä»¶å†…å®¹ã€‚

```bash
python train_ner.py \
  --train_file data/ner_train_large.jsonl \
  --output_dir models/ner \
  --epochs 5
```
*è¾“å‡ºï¼šæ¨¡å‹æ–‡ä»¶ä¿å­˜åœ¨ `models/ner/`*

### ç¬¬å››æ­¥ï¼šå¯¼å‡ºä¸ºç§»åŠ¨ç«¯æ¨¡å‹ (ONNX)
PyTorch æ¨¡å‹ä¸èƒ½ç›´æ¥åœ¨æ‰‹æœºä¸Šè¿è¡Œï¼Œæˆ‘ä»¬éœ€è¦å°†å…¶å¯¼å‡ºä¸ºé€šç”¨çš„ ONNX æ ¼å¼ã€‚

**1. å¯¼å‡ºåˆ†ç±»æ¨¡å‹**
```bash
python export_onnx.py \
  --model_dir models/classifier \
  --output models/classifier.onnx
```

**2. å¯¼å‡º NER æ¨¡å‹**
```bash
python export_ner_onnx.py \
  --model_dir models/ner \
  --output models/ner.onnx
```

*äº§ç‰©ï¼š`models/classifier.onnx` å’Œ `models/ner.onnx` (FP32ï¼Œçº¦ 390MB)*

### ç¬¬äº”æ­¥ï¼šæ¨¡å‹é‡åŒ– (Optimization) [æ¨è]
åŸå§‹å¯¼å‡ºçš„ ONNX æ¨¡å‹ä½“ç§¯è¾ƒå¤§ï¼ˆçº¦ 390MBï¼‰ï¼Œå»ºè®®è¿›è¡Œ Int8 é‡åŒ–ï¼Œå°†å…¶å‹ç¼©è‡³ 100MB å·¦å³ï¼Œæ›´é€‚åˆç§»åŠ¨ç«¯ä¸‹è½½å’ŒåŠ è½½ã€‚

**1. é‡åŒ–åˆ†ç±»æ¨¡å‹**
```bash
python clean_and_quantize.py models/classifier.onnx models/classifier.quant.onnx
```

**2. é‡åŒ– NER æ¨¡å‹**
```bash
python clean_and_quantize.py models/ner.onnx models/ner.quant.onnx
```

*äº§ç‰©ï¼š`models/classifier.quant.onnx` å’Œ `models/ner.quant.onnx` (Int8ï¼Œçº¦ 98MB)*

### ç¬¬å…­æ­¥ï¼šå…¨é“¾è·¯éªŒè¯ (Simulation)
åœ¨äº¤ä»˜ç»™ç§»åŠ¨ç«¯å¼€å‘ä¹‹å‰ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ `infer_pipeline.py` è„šæœ¬æ¥éªŒè¯ä¸¤ä¸ª ONNX æ¨¡å‹é…åˆå·¥ä½œçš„æ•ˆæœã€‚å®ƒæ¨¡æ‹Ÿäº†â€œå…ˆåˆ†ç±»ï¼Œå†æŠ½å–â€çš„ä¸šåŠ¡é€»è¾‘ã€‚

```bash
python infer_pipeline.py --text "æ˜å¤©ä¸‹åˆ3ç‚¹åœ¨ä¼šè®®å®¤å¼€ä¼š"
```

**é¢„æœŸè¾“å‡º JSON**:
```json
{
  "type": "æ—¥ç¨‹",
  "raw_text": "æ˜å¤©ä¸‹åˆ3ç‚¹åœ¨ä¼šè®®å®¤å¼€ä¼š",
  "time": "æ˜å¤© ä¸‹åˆ 3 ç‚¹",
  "location": "ä¼šè®®å®¤",
  "content": "å¼€ä¼š"
}
```

---

## ğŸ“± ç§»åŠ¨ç«¯é›†æˆæŒ‡å—

å¯¹äº Android (Java/Kotlin) æˆ– iOS (Swift/ObjC) å¼€å‘äººå‘˜ï¼š

1.  **è·å–æ¨¡å‹æ–‡ä»¶**ï¼š
    å°† `models/classifier.quant.onnx` å’Œ `models/ner.quant.onnx` æ”¾å…¥ App çš„ Assets ç›®å½•ã€‚

2.  **é›†æˆ ONNX Runtime åº“**ï¼š
    - Android: `implementation 'com.microsoft.onnxruntime:onnxruntime-android:1.15.0'`
    - iOS: `pod 'OnnxRuntime-c'`

3.  **æ¨ç†é€»è¾‘ä¼ªä»£ç **ï¼š
    ```java
    // 1. é¢„å¤„ç† (Tokenizer)
    // éœ€è¦åœ¨ç«¯ä¸Šå®ç°ä¸€ä¸ªç®€å•çš„ WordPiece Tokenizer (æˆ–ä½¿ç”¨ç°æˆåº“)
    long[] inputIds = tokenizer.encode(userInput);

    // 2. è¿è¡Œåˆ†ç±»æ¨¡å‹
    OrtSession clsSession = env.createSession("classifier.quant.onnx");
    float[] categoryLogits = clsSession.run(inputIds);
    int category = argmax(categoryLogits);

    if (category == SCHEDULE || category == ALARM) {
        // 3. è¿è¡Œ NER æ¨¡å‹
        OrtSession nerSession = env.createSession("ner.quant.onnx");
        float[][] tokenScores = nerSession.run(inputIds);
        int[] tags = argmax(tokenScores); // B-TIME, I-LOC...

        // 4. è§£æç»“æœ
        Result result = parseBioTags(userInput, tags);
        return result.toJson();
    }
    ```

## ğŸ æœ¬åœ°å¼€å‘ (ä¸ä½¿ç”¨ Docker)

å¦‚æœæ‚¨åšæŒä½¿ç”¨æœ¬åœ° Python ç¯å¢ƒï¼š

1. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ (Python 3.10+):
   ```bash
   python -m venv venv
   source venv/bin/activate
   ```

2. å®‰è£…ä¾èµ–:
   ```bash
   pip install -r requirements.txt
   ```

3. è¿è¡Œä¸Šè¿°æ‰€æœ‰ python å‘½ä»¤å³å¯ã€‚
